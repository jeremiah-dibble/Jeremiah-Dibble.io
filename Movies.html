<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>SVD Movie Ratings - Jeremiah Dibble</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	
	<body class="is-preload">
	
		<!-- Wrapper -->
			<div id="wrapper">
				<!-- Header -->
				<header id="header">
					<a href="index.html" class="logo">Jeremiah Dibble</a>
				</header>

				<!-- Nav -->
				<nav id="nav">
					<ul class="links">
						<li class="active"><a href="index.html">Projects</a></li>
						<li class="active"><a href="DNAS.html">SVD</a></li>
						<li class="active"><a href="Resume.html">Resume</a></li>

					</ul>
					<ul class="icons">
						<li><a href="https://www.linkedin.com/in/jeremiah-dibble/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
						<li><a href="https://github.com/jeremiah-dibble" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
					</ul>
				</nav>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<section class="post">
								<header class="major">
									<span class="date">Spring 2021</span>
									<h1>Predicting Movie <br /> Ratings (SVD)</h1>
									<h3>Summary</h3>
									<p>
										Singular Value Decomposition (SVD) can be used to learn the relationship between two groups. 
										Then the learned patterns can be used to predict interactions between a new member of either 
										group and the other group members. Here we use SVD to learn the relationship between viewers 
										movie ratings and the movies themselves. This allows us to predict how a view will feel about
										a movie based on previous responses.
									</p>
										<a href="https://github.com/jeremiah-dibble/Movie-Rating-Predictions">
											Check out the Code on my GitHub </a>
								</header>




								<div id="wrap">
									<input id="field" type="text" value="enter quality" size="20">
									<input type="button" value="Predict">
									<div id="results">
										Results</div>
								</div>
								<!-- end wrap -->
								<!-- <script type="text/javascript">
								function getLocation() {
									try {
									navigator.geolocation.getCurrentPosition(showPosition);
									} catch {
									resultsObj.innerHTM = err;
									}
								}
								
								function showPosition(position) {
									resultsObj.innerHTML =  "Location is: Latitude: " + fieldObj.value + position.coords.latitude + 
									"<br>Longitude: " + position.coords.longitude;
								}
									var resultsObj, fieldObj;
									resultsObj = document.getElementById("results");
									fieldObj = document.getElementById('field');
									fieldObj.addEventListener('blur', getLocation());
								</script> -->

								<script>
								import fetch from "node-fetch";
								async function query(data) {
									const response = await fetch(
										"https://api-inference.huggingface.co/models/gpt2",
										{
											headers: { Authorization: `Bearer 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` },
											method: "POST",
											body: JSON.stringify(data),
										}
									);
									const result = await response.json();
									const string = JSON.stringify(result.response)
									resultsObj.innerHTML = string
									return result;
								}
								var resultsObj, fieldObj;
									resultsObj = document.getElementById("results");
									fieldObj = document.getElementById('field');
									fieldObj.addEventListener('blur', query({inputs: fieldObj.value})())
								query({inputs: fieldObj.value}).then((response) => {
									console.log(JSON.stringify(response));
								});
								</script>











								<!-- <canvas id="myChart" width="400" height="400"></canvas> -->
								<!-- <script>
									var ctx = document.getElementById("myChart").getContext('2d');
								
									var testOutput = document.getElementById("welcome").value;
								
									var testValue1 = document.getElementById("test1").value;
									var testValue2 = document.getElementById("test2").value;
									var testValue3 = document.getElementById("test3").value;
									var testValue4 = document.getElementById("test4").value;
									var myChart = new Chart(ctx, {
										type: 'bar',
										data: {
											labels: ["Red", "Blue", "Yellow", "Green", "Purple", "Orange"],
											datasets: [{
												label: '# of Votes',
												data: [testValue1, testValue2, testValue3, testValue4],
												backgroundColor: [
													'rgba(255, 99, 132, 0.2)',
													'rgba(54, 162, 235, 0.2)',
													'rgba(255, 206, 86, 0.2)',
													'rgba(75, 192, 192, 0.2)',
													'rgba(153, 102, 255, 0.2)',
													'rgba(255, 159, 64, 0.2)'
												],
												borderColor: [
													'rgba(255,99,132,1)',
													'rgba(54, 162, 235, 1)',
													'rgba(255, 206, 86, 1)',
													'rgba(75, 192, 192, 1)',
													'rgba(153, 102, 255, 1)',
													'rgba(255, 159, 64, 1)'
												],
												borderWidth: 1
											}]
										},
										options: {
											scales: {
												yAxes: [{
													ticks: {
														beginAtZero:true
													}
												}]
											}
										}
									});
								</script> -->


								<h2>1 &emsp; Basic Visualizations</h2>
								<p>
									In the basic visualization of all ratings of the MovieLens Dataset we noticed that the distribution is
									positively skewed, with more movies having a rating greater than 3, than movies with ratings less than
									3. Furthermore, the most popular rating was a 4/5 compared to a more normal distribution where we
									would expect the most common rating to be a 3/5. The positive skew suggests that most movies are
									generally enjoyed, which may be indicative of the tendency of viewers to watch movies that tend to align
									with interest or genre preference.

								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_allMovies.png" />
									<p><b>Figure 1: All Ratings of MovieLens Dataset</p></b>
									</header>
								<p>In the basic visualization of all the ratings of the 10 most popular movies we noticed that the most 
									to the ratings are positive which was expected, but we also see that there are a handful of the most 
									popular movies that are not rated well. We also need to keep in mind this is likely caused by some 
									of the most popular movies being watched based on the popularity factor instead of aligning with the 
									specific views interest as directly.

								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_mostRated.png" />
									<p><b>Figure 2: All Ratings of 10 Most Popular (Most Rated) Moviest</p></b>
	
								</header>
								<p>In the basic visualization of all the ratings of the 10 best movies we noticed that the ratings of 
									the top rated movies are all movies with one rating of 5, so we should keep number of ratings in mind.

								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_HighestRated.png" />
									<p><b>Figure 3: All Ratings of 10 Best (Highest Rated) Movies</p></b>
	
								</header>
								<p>In the basic visualization of all the ratings of the Action, Crime, and Fantasy genres we noticed 
									that these genres seem to have roughly the same distribution despite the difference in size.
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_genre.png" />
									<p><b>Figure 4: All Ratings of Action, Crime, and Fantasy Movies</p></b>
	
								</header>




								<h2>2 &emsp; Matrix Factorization Visualizations</h2>
								<p>We implemented and created visuals for three different methods.</p>
								<h3>2.1 &ensp; Matrix Factorization Methods</h3>
								<h4>2.1.1 &ensp; Using Barebones SVD </h4>
								<p>
									First we implementedthe barebones svd with k=20 and a range of regularizations to determine 
									which had the lowest validation error. We found the optimal regularization to be 0.1. 
									This produced a mean squared error in predicting Y of 1.22, which we would discover makes 
									it the worst of the bunch. The homework 5 code factorized the matrix by randomly filling U 
									and Y based on a given k. Then U and Y are updated using stochastic gradient decent and with 
									the sum of Frobenius norms used for regularization. The regularization will keep the overall 
									weights in the matrix lower to reduce over-fitting. This model only used SGD on U and Y  and 
									included the normalization in the error calculations.
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_5_U.png" />
									<p><b>Figure 5: Latent Factors of U projection of barebones SGD Model</p></b>
									<img src="images/movie_ratings/miniproject2_5_V.png" />
									<p><b>Figure 6: Latent Factors of V projection of barebones SGD Model</p></b>
								</header>
								
								<h4>2.1.2 &ensp; Using the Surprise Package (off the self matrix factorization) With Bias </h4>
								<p>
									Next we used the Surprise Package to conduct the matrix factorization without a bias term. 
									The Surprise package uses a more complex version of SGD with an included row and column 
									bias term. These bias terms compensate for the individuality of particular people or movies, 
									for example if someone tends to rate every movie higher on average that will be compensated 
									from in the bias term. This model performed much better on the test set than the homework 5 
									implementation which indicates that the bias terms helped create a more accurate model. 
									The test mean squared error was .871.
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_sur_SVD_U.png" />
									<p><b>Figure 7: Latent Factors of U projection of Surprise with Bias SGD Model</p></b>
									<img src="images/movie_ratings/miniproject2_sur_SVD_V.png" />
									<p><b>Figure 8: Latent Factors of V projection of Surprise with Bias SGD Model</p></b>
								</header>

								<h4>2.1.3 &ensp; Using the Surprise Package with Non-negative Matrix Factorization </h4>
								<p>
									Lastly we used the Surprise Package again but this time with non-negative matrix factorization 
									(NMF). As the name implies this model only produces positive factor values, which requires 
									positive initialization and a more complex method of SGD. Keeping all the factor values positive 
									could be seen as a prudent move given we are modeling rankings of movies, which cannot be negative, 
									but this model failed to out perform the basic Surprise Package with bias. The NMF may also have 
									under-performed because it is highly sensitive to initial conditions. Lastly, we chose not to use 
									bias on the NMF model because adding a bias term make the model highly prone to over-fitting. 
									This model ranked in the middle of the of the pack on the test set, with a mean squared error of .921.
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_sur_NMF_U.png" />
									<p><b>Figure 9: Latent Factors of U projection of Surprise with NMF Model</p></b>
									<img src="images/movie_ratings/miniproject2_sur_NMF_V.png" />
									<p><b>Figure 10: Latent Factors of V projection of Surprise with NMF Model</p></b>
								</header>

								<h4>2.1.4 &ensp; Conclusion and Best Model </h4>
								<p>
									Based on the mean squared errors on the test set we can determine the relative quality of the 
									three models we implemented. As already discussed, the Surprise Package with bias outperformed 
									all of the other models (.871); followed by the NMF model (.921) and the barebones code in last place 
									(1.22). From the mean squared errors we can see that most significant change was moving to the 
									Surprise Package, which accounted for the vast majority of the reduction in error from first to 
									last place. The NMF model performed roughly 5\% worse than the bias model, which is seemingly 
									small but the Netflix competition was only for a 10\% increase. Thus we can conclude that the 
									Surprise model with bias is meaningfully better than the other models.
								</p>
								<h3>2.2 &ensp; Matrix Factorization Visualization </h3>
								<h4>2.2.1 &ensp; Latent Factors of V Projection with 10 Random Films </h4>
								
								<p>
									Here, we compare the 1st two principal components of the latent factors in the V projection. 
									We observe that along the 1st principal component, the movies tend to be positioned relatively 
									close to each other, with movies such as "Blood for Dracula" being on the far left and movies 
									such as "Dirty Dancing" being near the right-middle. Along the 2nd principal component, we see 
									that the same phenomenon is observed, with movies such as "Dirty Dancing" residing in the lower 
									half and movies such as "Father of the Bride" residing the upper half. However, there are a 
									non-negligible number of outliers which are occasionally different between models. We further 
									observe a rough similarity in the plots between the unbiased SGD and biased SVD models, which 
									makes sense given that the models produce similar latent factors given they both perform ordinary 
									matrix factorization algorithms. However, the NMF model tended to produce much different estimates 
									with comparatively much lower variance along the 1st principal component, likely due to its latent 
									factors being forced non-negative. 
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_5_V_vis_rand.png" />
									<p><b>Figure 11: Latent Factors of V projection barbones svd with 10 random films</p></b>
									<img src="images/movie_ratings/miniproject2_sur_SVD_V_vis_rand.png" />
									<p><b>Figure 12: Latent Factors of V projection of Surprise with SVD Model using 10 random films</p></b>
									<img src="images/movie_ratings/miniproject2_sur_NMF_V_vis_rand.png" />
									<p><b>Figure 13: Latent Factors of V projection of Surprise with NMF Model using 10 random films</p></b>
								</header>

								<h4>2.2.2 &ensp; Latent Factors of V Projection with 10 Most Popular Films </h4>
								
								<p>
									The 3 visualizations of the 10 most popular films are fairly similar, with a stronger concentration in 
									the middle to top right. The better models (bias and NMF) spread out the movies more evenly than the 
									model in barebones. Most movies remained roughly in the same region as their counterparts in the other models, 
									but certain movies like 'Four Rooms' drifted dramatically from having one of the lowest 2nd principal 
									components to having one of the highest. Further, we observe much higher clustering in the plotted points 
									in these popular movies compared to the plots of random movies. In the unbiased SGD model, we see the 
									movies tended to cluster around higher values of the 1st/2nd principal components. In the unbiased SVD 
									model, we see that the components had higher variance and were less clustered in a particular region, 
									suggesting the bias term helped absorb a value correlated to the overall popularity of the movie. In the
									NMF model, we see similarity to the unbiased SVD model, with the points being well-distributed across the 
									axis of the 1st principal component. 
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_5_V_vis_pop.png" />
									<p><b>Figure 14: Latent Factors of V projection barebones code with 10 most popular films</p></b>
									<img src="images/movie_ratings/miniproject2_sur_SVD_V_vis_pop.png" />
									<p><b>Figure 15: Latent Factors of V projection of Surprise with SVD Model using 10 most popular films</p></b>
									<img src="images/movie_ratings/miniproject2_sur_NMF_V_vis_pop.png" />
									<p><b>Figure 16: Latent Factors of V projection of Surprise with NMF Model using 10 most popular films</p></b>
								</header>
								
								<h4>2.2.3 &ensp; Latent Factors of V Projection with 10 Most Best Rated Films </h4>
								
								<p>
									The best rated films had very few rating relative to the others, with all of the ratings being perfect 5s. 
									We can see in the barebones model almost all of the movies have a maximal  PC 1 expect 'The Favor.' As we move 
									up the quality of the models the distributions of the latent factors increases. This is similar to what 
									occurred int he most popular films category but it is far more dramatic. This was likely caused by the 
									smaller number of rating for the best rated films when compared to the most rated films, so the better 
									models compensated for the lack of data more effectively which causes the more dramatic improvement in 
									best films.
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_5_V_vis_best.png" />
									<p><b>Figure 17: Latent Factors of V projection barebones code with 10 best films</p></b>
									<img src="images/movie_ratings/miniproject2_sur_SVD_V_vis_best.png" />
									<p><b>Figure 18: Latent Factors of V projection of Surprise with SVD Model using 10 best films</p></b>
									<img src="images/movie_ratings/miniproject2_sur_NMF_V_vis_best.png" />
									<p><b>Figure 19: Latent Factors of V projection of Surprise with NMF Model using 10 best films</p></b>
								</header>

								<h4>2.2.4 &ensp; Latent Factors of V For Different Genres </h4>
								
								<p>
									Looking at the following plots we can see that all the genres seem to follow a more linear pattern with respect to the principle components in the Surprise with NMF model. We also noticed the plots for barebones and Surprise Biased SVD looked very similar as well. Furthermore, we also notice that there doesn't appear to be genre based clustering or any particular pattern with respect to each genre. The lack of clustering or pattern within genres was a little surprising, but it makes sense because there are good and bad movie ratings for all genres. Overall the visualizations for each genre, Action, Crime, and Fantasy, look very similar and there was no distinguishable characteristics between them in the plots.
								</p>
								<header class="major">
									<img src="images/movie_ratings/miniproject2_5_V_genre.png" />
									<p><b>Figure 20: Latent Factors of V projection barebones code with different genres</p></b>
									<img src="images/movie_ratings/miniproject2_sur_SVD_V_genre.png" />
									<p><b>Figure 21: Latent Factors of V projection of Surprise with SVD Model with different genres</p></b>
									<img src="images/movie_ratings/miniproject2_sur_NMF_V_genre.png" />
									<p><b>Figure 22: Latent Factors of V projection of Surprise with NMF Model with different genres</p></b>
								</header>


							</section>

					</div>

					

				<!-- Footer -->
				<footer id="footer">

					<section class="split contact">
						<section class="alt">
							<h3>Address</h3>
							<p>1401 Manhattan Beach Blvd.<br />
							Manhattan Beach, CA 90291</p>
						</section>
						<section>
							<h3>Phone</h3>
							<p><a href="#">(424) 216-1785</a></p>
						</section>
						<section>
							<h3>Email</h3>
							<p><a href="#">jeremiah.dibble@gmail.com</a></p>
						</section>
						<section>
							<h3>Social</h3>
							<ul class="icons alt">
								<li><a href="https://www.linkedin.com/in/jeremiah-dibble/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
								<li><a href="https://github.com/jeremiah-dibble" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
					</section>
				</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>